{"cells":[{"cell_type":"markdown","metadata":{"id":"V1fmPlzy8Npi"},"source":["# Tutorial 10: Adversarial attacks"]},{"cell_type":"markdown","metadata":{"id":"_k90Sv6u8Npk"},"source":["Credits:\n","* UVA Deep Learning\n","* Adversarial Patch (https://machine-learning-and-security.github.io/papers/mlsec17_paper_27.pdf)\n","* \"Explaining and harnessing adversarial examples.\" ICLR 2015.\n","* \"Universal adversarial perturbations against semantic image segmentation\"\n","* \"Breaking neural networks with adversarial attacks.\" (https://towardsdatascience.com/breaking-neural-networks-with-adversarial-attacks-f4290a9a45aa)"]},{"cell_type":"markdown","source":["# Run Below\n","Run the code below to set up your prereqs for the patch attack!"],"metadata":{"id":"wBpOZ_e7-XCn"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZCrbFHFk8Npk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714613654596,"user_tz":420,"elapsed":92395,"user":{"displayName":"Savannah Alanis","userId":"15830036764820376605"}},"outputId":"d5ebd604-0d02-4f9c-bc58-a2618289b38c"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-1-bf57b8178c50>:11: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n","  set_matplotlib_formats('svg', 'pdf') # For export\n","INFO:lightning_fabric.utilities.seed:Seed set to 42\n"]},{"output_type":"stream","name":"stdout","text":["Using device cuda:0\n"]}],"source":["## Standard libraries\n","import os\n","import json\n","import math\n","import time\n","import numpy as np\n","import scipy.linalg\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","from IPython.display import set_matplotlib_formats\n","set_matplotlib_formats('svg', 'pdf') # For export\n","from matplotlib.colors import to_rgb\n","import matplotlib\n","matplotlib.rcParams['lines.linewidth'] = 2.0\n","import seaborn as sns\n","sns.set()\n","from tqdm.notebook import tqdm\n","\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.utils.data as data\n","from torch.utils.data import DataLoader\n","import torch.optim as optim\n","import torchvision\n","from torchvision.datasets import CIFAR10\n","from torchvision.datasets import GTSRB\n","from torchvision import transforms\n","\n","\n","try:\n","    import pytorch_lightning as pl\n","except ModuleNotFoundError:\n","    !pip install --quiet pytorch-lightning>=1.4\n","    import pytorch_lightning as pl\n","from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n","\n","\n","DATASET_PATH = \"../data\"\n","CHECKPOINT_PATH = \"../saved_models/tutorial10\"\n","pl.seed_everything(42)\n","\n","# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","\n","device = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda:0\")\n","print(\"Using device\", device)"]},{"cell_type":"markdown","metadata":{"id":"CVLwGQQh8Npl"},"source":["Downloading dataset and model (GTSRB), creating DataLoader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nOOMujLq8Npl"},"outputs":[],"source":["# Directory path settings\n","DATASET_PATH = 'datasets'\n","CHECKPOINT_PATH = 'checkpoints'\n","\n","# Create dataset and checkpoint directories if they don't exist\n","os.makedirs(DATASET_PATH, exist_ok=True)\n","os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n","\n","# Image normalization using ImageNet statistics\n","NORM_MEAN = np.array([0.485, 0.456, 0.406])\n","NORM_STD = np.array([0.229, 0.224, 0.225])\n","\n","# Transformations applied to each image\n","plain_transforms = transforms.Compose([\n","    transforms.Resize((224, 224)),  # Resize images for ResNet50 input\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=NORM_MEAN, std=NORM_STD)\n","])\n","\n","# Load the GTSRB dataset\n","dataset = GTSRB(root=DATASET_PATH, split='train', download=True, transform=plain_transforms)\n","\n","# Data loader configuration\n","data_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=0)\n","\n","label_names = [\n","    \"Speed limit (20km/h)\", \"Speed limit (30km/h)\", \"Speed limit (50km/h)\",\n","    \"Speed limit (60km/h)\", \"Speed limit (70km/h)\", \"Speed limit (80km/h)\",\n","    \"End of speed limit (80km/h)\", \"Speed limit (100km/h)\", \"Speed limit (120km/h)\",\n","    \"No passing\", \"No passing for vehicles over 3.5 metric tons\",\n","    \"Right-of-way at the next intersection\", \"Priority road\", \"Yield\", \"Stop\",\n","    \"No vehicles\", \"Vehicles over 3.5 metric tons prohibited\", \"No entry\",\n","    \"General caution\", \"Dangerous curve to the left\", \"Dangerous curve to the right\",\n","    \"Double curve\", \"Bumpy road\", \"Slippery road\", \"Road narrows on the right\",\n","    \"Road work\", \"Traffic signals\", \"Pedestrians\", \"Children crossing\",\n","    \"Bicycles crossing\", \"Beware of ice/snow\", \"Wild animals crossing\",\n","    \"End of all speed and passing limits\", \"Turn right ahead\", \"Turn left ahead\",\n","    \"Ahead only\", \"Go straight or right\", \"Go straight or left\", \"Keep right\",\n","    \"Keep left\", \"Roundabout mandatory\", \"End of no passing\", \"End of no passing by vehicles over 3.5 metric tons\"\n","]"]},{"cell_type":"markdown","metadata":{"id":"jGPpcCzN8Npl"},"source":["## Setting up the Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DO2uMIeY8Npm"},"outputs":[],"source":["from torchvision.models import resnet50\n","from google.colab import drive\n","drive.mount(\"/content/drive\")\n","model = resnet50()\n","model.fc = torch.nn.Linear(2048, 43)\n","model.load_state_dict(torch.load(\"/content/drive/Shareddrives/ACM at UCLA/2023-24/Cyber/Spring 2024/Cyber Lab/Cyber x AI Colab - Patch Attack/resnet50_gtsrb.pth\"))\n","\n","pretrained_model = model\n","pretrained_model = pretrained_model.to(device)\n","\n","# No gradients needed for the network\n","pretrained_model.eval()\n","for p in pretrained_model.parameters():\n","    p.requires_grad = False"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"PDvM8Bzh83c8","executionInfo":{"status":"error","timestamp":1714613884663,"user_tz":420,"elapsed":5776,"user":{"displayName":"Savannah Alanis","userId":"15830036764820376605"}},"outputId":"edfaa962-7255-453d-aef3-adfa2b3f0932","colab":{"base_uri":"https://localhost:8080/","height":304}},"execution_count":null,"outputs":[{"output_type":"error","ename":"MessageError","evalue":"Error: credential propagation was unsuccessful","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    131\u001b[0m   )\n\u001b[1;32m    132\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"]}]},{"cell_type":"markdown","metadata":{"id":"L7pQ4aEb8Npm"},"source":["Eval for sanity check\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EWNf-g0O8Npm"},"outputs":[],"source":["def eval_model(dataset_loader, img_func=None):\n","    tp, tp_5, counter = 0., 0., 0.\n","    for imgs, labels in tqdm(dataset_loader, desc=\"Validating...\"):\n","        imgs = imgs.to(device)\n","        labels = labels.to(device)\n","        if img_func is not None:\n","            imgs = img_func(imgs, labels)\n","        with torch.no_grad():\n","            preds = pretrained_model(imgs)\n","        tp += (preds.argmax(dim=-1) == labels).sum()\n","        tp_5 += (preds.topk(5, dim=-1)[1] == labels[...,None]).any(dim=-1).sum()\n","        counter += preds.shape[0]\n","    acc = tp.float().item()/counter\n","    top5 = tp_5.float().item()/counter\n","    print(f\"Top-1 error: {(100.0 * (1 - acc)):4.2f}%\")\n","    print(f\"Top-5 error: {(100.0 * (1 - top5)):4.2f}%\")\n","    return acc, top5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r1TM8koB8Npm"},"outputs":[],"source":["_ = eval_model(data_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PnD00G7N8Npo"},"outputs":[],"source":["def show_prediction(img, label, pred, K=5, adv_img=None, noise=None):\n","\n","    if isinstance(img, torch.Tensor):\n","        # Tensor image to numpy\n","        img = img.cpu().permute(1, 2, 0).numpy()\n","        img = (img * NORM_STD[None,None]) + NORM_MEAN[None,None]\n","        img = np.clip(img, a_min=0.0, a_max=1.0)\n","        label = label.item()\n","\n","    # Plot on the left the image with the true label as title.\n","    # On the right, have a horizontal bar plot with the top k predictions including probabilities\n","    if noise is None or adv_img is None:\n","        fig, ax = plt.subplots(1, 2, figsize=(10,2), gridspec_kw={'width_ratios': [1, 1]})\n","    else:\n","        fig, ax = plt.subplots(1, 5, figsize=(12,2), gridspec_kw={'width_ratios': [1, 1, 1, 1, 2]})\n","\n","    ax[0].imshow(img)\n","    ax[0].set_title(label_names[label])\n","    ax[0].axis('off')\n","\n","    if adv_img is not None and noise is not None:\n","        # Visualize adversarial images\n","        adv_img = adv_img.cpu().permute(1, 2, 0).numpy()\n","        adv_img = (adv_img * NORM_STD[None,None]) + NORM_MEAN[None,None]\n","        adv_img = np.clip(adv_img, a_min=0.0, a_max=1.0)\n","        ax[1].imshow(adv_img)\n","        ax[1].set_title('Adversarial')\n","        ax[1].axis('off')\n","        # Visualize noise\n","        noise = noise.cpu().permute(1, 2, 0).numpy()\n","        noise = noise * 0.5 + 0.5 # Scale between 0 to 1\n","        ax[2].imshow(noise)\n","        ax[2].set_title('Noise')\n","        ax[2].axis('off')\n","        # buffer\n","        ax[3].axis('off')\n","\n","    if abs(pred.sum().item() - 1.0) > 1e-4:\n","        pred = torch.softmax(pred, dim=-1)\n","    topk_vals, topk_idx = pred.topk(K, dim=-1)\n","    topk_vals, topk_idx = topk_vals.cpu().numpy(), topk_idx.cpu().numpy()\n","    ax[-1].barh(np.arange(K), topk_vals*100.0, align='center', color=[\"C0\" if topk_idx[i]!=label else \"C2\" for i in range(K)])\n","    ax[-1].set_yticks(np.arange(K))\n","    ax[-1].set_yticklabels([label_names[c] for c in topk_idx])\n","    ax[-1].invert_yaxis()\n","    ax[-1].set_xlabel('Confidence')\n","    ax[-1].set_title('Predictions')\n","\n","    plt.show()\n","    plt.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pR-mNno28Npo"},"outputs":[],"source":["exmp_batch, label_batch = next(iter(data_loader))\n","with torch.no_grad():\n","    preds = pretrained_model(exmp_batch.to(device))\n","for i in range(1,17,5):\n","    show_prediction(exmp_batch[i], label_batch[i], preds[i])"]},{"cell_type":"markdown","source":["# Patch Attack\n","Your code goes here!"],"metadata":{"id":"eJTmuAOW-cCz"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"vEmWnsMR8Npp"},"outputs":[],"source":["##### YOUR CODE BELOW #####\n","def place_patch(img, patch):\n","    for i in range(img.shape[0]):\n","        # Define a horizontal and vertical offset at random, and modify your img with the patch at that offset\n","    # return img\n","##### END CODE #####"]},{"cell_type":"markdown","metadata":{"id":"KVBvP6qP8Npp"},"source":["The patch itself will be an `nn.Parameter` whose values are in the range between -infinity to infinity, so we want to map these to the range of valid pixel values in an image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vsh43i-M8Npp"},"outputs":[],"source":["TENSOR_MEANS, TENSOR_STD = torch.FloatTensor(NORM_MEAN)[:,None,None], torch.FloatTensor(NORM_STD)[:,None,None]\n","def patch_forward(patch):\n","    # Map patch values from [-infty,infty] to ImageNet min and max\n","    patch = (torch.tanh(patch) + 1 - 2 * TENSOR_MEANS) / (2 * TENSOR_STD)\n","    return patch"]},{"cell_type":"markdown","metadata":{"id":"pn9qdZ4W8Npp"},"source":["We've provided the below evaluation code for your patch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lqB4WiPa8Npp"},"outputs":[],"source":["##### YOUR CODE BELOW #####\n","def eval_patch(model, patch, val_loader, target_class):\n","    model.eval()\n","    tp, tp_5, counter = 0., 0., 0.\n","    with torch.no_grad():\n","        for img, img_labels in tqdm(val_loader, desc=\"Validating...\", leave=False):\n","            # For stability, place the patch at 4 random locations per image, and average the performance\n","            for _ in range(4):\n","                pred = # TODO: Place patch on the image and get a prediction!\n","\n","                # Note: in the accuracy calculation, we need to exclude the images that are of our target class\n","                # as we would not \"fool\" the model into predicting those\n","                tp += torch.logical_and(pred.argmax(dim=-1) == target_class, img_labels != target_class).sum()\n","                tp_5 += torch.logical_and((pred.topk(5, dim=-1)[1] == target_class).any(dim=-1), img_labels != target_class).sum()\n","                counter += (img_labels != target_class).sum()\n","    acc = tp/counter\n","    top5 = tp_5/counter\n","    return acc, top5\n","##### END CODE #####"]},{"cell_type":"markdown","metadata":{"id":"5ddgrf818Npp"},"source":["### TODO: insert your code below to create a patch attack!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PNsCogzI8Npp"},"outputs":[],"source":["##### YOUR CODE BELOW #####\n","def patch_attack(model, target_class, patch_size=64, num_epochs=5):\n","    # Get dataset split\n","    total_size = #\n","    train_size = #\n","    val_size = #\n","    train_set, val_set = # get a random data split\n","\n","    train_loader = data.DataLoader(train_set, batch_size=32, shuffle=True, drop_last=True, num_workers=8)\n","    val_loader = data.DataLoader(val_set, batch_size=32, shuffle=False, drop_last=False, num_workers=4)\n","\n","\n","    # Create parameter and optimizer\n","    if not isinstance(patch_size, tuple):\n","        patch_size = (patch_size, patch_size)\n","    patch = # create the patch as an nn.Parameter\n","    optimizer = # choose an optimizer\n","    criterion = # choose a criterion (to calculate loss)\n","\n","    # Training loop\n","    for epoch in range(num_epochs):\n","        t = tqdm(train_loader, leave=False)\n","        for img, _ in t:\n","            img = # place patch on imag\n","            img = # move image to device\n","            pred = # get predictions\n","            labels = torch.zeros(img.shape[0], device=pred.device, dtype=torch.long).fill_(target_class)\n","\n","            # do loss calculations (with respect to labels), and step optimizer of patch as before\n","            t.set_description(f\"Epoch {epoch}, Loss: {loss.item():4.2f}\")\n","\n","    # Final validation\n","    acc, top5 = eval_patch(model, patch, val_loader, target_class)\n","\n","    return patch.data, {\"acc\": acc.item(), \"top5\": top5.item()}\n","##### END CODE HERE #####"]},{"cell_type":"markdown","metadata":{"id":"Mq0kK3bg8Npp"},"source":["Note: training the patches above can take a few mins"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i4HV2IMb8Npp"},"outputs":[],"source":["# when you train new patches, you can save the results via calling this function\n","def save_results(patch_dict):\n","    result_dict = {cname: {psize: [t.item() if isinstance(t, torch.Tensor) else t\n","                                   for t in patch_dict[cname][psize][\"results\"]]\n","                           for psize in patch_dict[cname]}\n","                   for cname in patch_dict}\n","    with open(os.path.join(CHECKPOINT_PATH, \"patch_results.json\"), \"w\") as f:\n","        json.dump(result_dict, f, indent=4)"]},{"cell_type":"markdown","metadata":{"id":"4vnCYcQI8Npp"},"source":["Below is a provided function to train and evaluate the patches using the code you defined above."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s6t1sV2U8Npp"},"outputs":[],"source":["def get_patches(class_names, patch_sizes):\n","    result_dict = dict()\n","\n","    # Loop over all classes and patch sizes\n","    for name in class_names:\n","        result_dict[name] = dict()\n","        for patch_size in patch_sizes:\n","            c = label_names.index(name)\n","            file_name = os.path.join(CHECKPOINT_PATH, f\"{name}_{patch_size}_patch.pt\")\n","            # Load patch if pretrained file exists, otherwise start training\n","            if not os.path.isalfile(file_name):\n","                patch, val_results = patch_attack(pretrained_model, target_class=c, patch_size=patch_size, num_epochs=5)\n","                print(f\"Validation results for {name} and {patch_size}:\", val_results)\n","                torch.save(patch, file_name)\n","            else:\n","                patch = torch.load(file_name)\n","            # Load evaluation results if exist, otherwise manually evaluate the patch\n","            if name in json_results:\n","                results = json_results[name][str(patch_size)]\n","            else:\n","                results = eval_patch(pretrained_model, patch, data_loader, target_class=c)\n","\n","            # Store results and the patches in a dict for better access\n","            result_dict[name][patch_size] = {\n","                \"results\": results,\n","                \"patch\": patch\n","            }\n","\n","    return result_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B1wl4vpu8Npp"},"outputs":[],"source":["class_names = # Fill in!\n","patch_sizes = [32, 48, 64]\n","\n","patch_dict = get_patches(class_names, patch_sizes)\n","# save_results(patch_dict) # Uncomment if you add new class names and want to save the new results"]},{"cell_type":"markdown","metadata":{"id":"Wb3Oxr9_8Nps"},"source":["Before looking at the quantitative results, we can actually visualize the patches.\n","\n","You've just written your first physical attack! Now, try printing these out and seeing if you can get them to misclassify the stop sign!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OsmFwtBG8Nps"},"outputs":[],"source":["def show_patches():\n","    fig, ax = plt.subplots(len(patch_sizes), len(class_names), figsize=(len(class_names)*2.2, len(patch_sizes)*2.2))\n","    for c_idx, cname in enumerate(class_names):\n","        for p_idx, psize in enumerate(patch_sizes):\n","            patch = patch_dict[cname][psize][\"patch\"]\n","            patch = (torch.tanh(patch) + 1) / 2 # Parameter to pixel values\n","            patch = patch.cpu().permute(1, 2, 0).numpy()\n","            patch = np.clip(patch, a_min=0.0, a_max=1.0)\n","            ax[p_idx][c_idx].imshow(patch)\n","            ax[p_idx][c_idx].set_title(f\"{cname}, size {psize}\")\n","            ax[p_idx][c_idx].axis('off')\n","    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n","    plt.show()\n","show_patches()"]},{"cell_type":"markdown","metadata":{"id":"ujv0xOBV8Nps"},"source":["## Evaluation Code\n","Credit: UVA Deep Learning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GbkZrL-x8Nps"},"outputs":[],"source":["%%html\n","<!-- Some HTML code to increase font size in the following table -->\n","<style>\n","th {font-size: 120%;}\n","td {font-size: 120%;}\n","</style>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-b9B5m2E8Nps"},"outputs":[],"source":["import tabulate\n","from IPython.display import display, HTML\n","\n","def show_table(top_1=True):\n","    i = 0 if top_1 else 1\n","    table = [[name] + [f\"{(100.0 * patch_dict[name][psize]['results'][i]):4.2f}%\" for psize in patch_sizes]\n","             for name in class_names]\n","    display(HTML(tabulate.tabulate(table, tablefmt='html', headers=[\"Class name\"] + [f\"Patch size {psize}x{psize}\" for psize in patch_sizes])))"]},{"cell_type":"markdown","metadata":{"id":"ihg00xa98Npt"},"source":["First, we will create a table of top-1 accuracy, meaning that how many images have been classified with the target class as highest prediction?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IjGW2TsJ8Npt"},"outputs":[],"source":["show_table(top_1=True)"]},{"cell_type":"markdown","metadata":{"id":"PjTQrPGH8Npt"},"source":["The clear trend, that we would also have expected, is that the larger the patch, the easier it is to fool the model. For the largest patch size of $64\\times 64$, we are able to fool the model on almost all images, despite the patch covering only 8% of the image. The smallest patch actually covers 2% of the image, which is almost neglectable. Still, the fooling accuracy is quite remarkable. A large variation can be however seen across classes. While *school bus* and *pineapple* seem to be classes that were easily predicted, *toaster* and *lipstick* seem to be much harder for creating a patch. It is hard to intuitively explain why our patches underperform on those classes. Nonetheless, a fooling accuracy of >40% is still very good for such a tiny patch.\n","\n","Let's also take a look at the top-5 accuracy:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ubWGTOrR8Npt"},"outputs":[],"source":["show_table(top_1=False)"]},{"cell_type":"markdown","metadata":{"id":"3Uq76tRE8Npt"},"source":["We see a very similar pattern across classes and patch sizes. The patch size $64$ obtains >99.7% top-5 accuracy for any class, showing that we can almost fool the network on any image. A top-5 accuracy of >70% for the hard classes and small patches is still impressive and shows how vulnerable deep CNNs are to such attacks.\n","\n","Finally, let's create some example visualizations of the patch attack in action."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X5ymxZyU8Npt"},"outputs":[],"source":["def perform_patch_attack(patch):\n","    patch_batch = exmp_batch.clone()\n","    patch_batch = place_patch(patch_batch, patch)\n","    with torch.no_grad():\n","        patch_preds = pretrained_model(patch_batch.to(device))\n","    for i in range(1,17,5):\n","        show_prediction(patch_batch[i], label_batch[i], patch_preds[i])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BtMCemMF8Npt"},"outputs":[],"source":["perform_patch_attack(patch_dict['goldfish'][32]['patch'])"]},{"cell_type":"markdown","metadata":{"id":"ai5_GBX48Npt"},"source":["The tiny goldfish patch can change all of the predictions to \"goldfish\" as top class. Note that the patch attacks work especially well if the input image is semantically similar to the target class (e.g. a fish and the target class \"goldfish\" works better than an airplane image with that patch). Nevertheless, we can also let the network predict semantically dis-similar classes by using a larger patch:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_8zK64oJ8Npt"},"outputs":[],"source":["perform_patch_attack(patch_dict['school bus'][64]['patch'])"]},{"cell_type":"markdown","metadata":{"id":"DdYQ6Tnz8Npt"},"source":["Although none of the images have anything to do with an American school bus, the high confidence of often 100% shows how powerful such attacks can be. With a few lines of code and access to the model, we were able to generate patches that we add to any image to make the model predict any class we want."]},{"cell_type":"markdown","metadata":{"id":"MU2UpPg08Npu"},"source":["## Conclusion\n","\n","In this tutorial, we have looked at different forms of adversarial attacks. Deep CNNs can be fooled by only slight modifications to the input. Whether it is a carefully designed noise pattern, unnoticeable for a human, or a small patch, we are able to manipulate the networks' predictions significantly. The fact that even white-box attacks can be transferable across networks, and that there exist no suitable protections against all possible adversarial attacks, make this concept a massive problem for real-world applications. While adversarial attacks can also be used for improving/training a robust model or a GAN, it is not close to being solved yet. This is also because neural networks are currently complex, unknown non-linear functions in high-dimensional looking for correlations instead of causation. In the next years, we might hopefully see an improvement in the stability of such models by using causal approaches and/or introducing uncertainty."]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.2"},"colab":{"provenance":[{"file_id":"1FJ2sycPqPswIJLixuXIaG-Ua43cfLKym","timestamp":1715217819478},{"file_id":"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial10/Adversarial_Attacks.ipynb","timestamp":1713405749357}],"machine_shape":"hm","gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}